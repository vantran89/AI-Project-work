---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- #region -->
# Group F

Groupmembers are:


|Gender|Firstname|Surname|Email| Study program| Semester| Student ID|
|:----:|:--------|:------|:----|:----------|:------------|:--------|
| f | Cam Van | Tran Thi | cam.tranthi@student.uni-siegen.de | HCI | 4 | 1542660 |
| f |.....|.....|.....| HCI |.....|.....|
| m |.....|.....|.....| HCI |.....|.....|

    
**Course**: First steps into Artificial Intelligence: Organization & Introduction

**Examiner**: Prof. Dr. Thomas Ludwig

**Semester**: Summersemester 2022

**Submission date**: 31. July 2022

**City**: Siegen, Germany

<!-- #endregion -->

## Imports
Following imports and addtional installtions are needed:

```{python}
# If there are additional libraries needed to run the following code install them here

```

```{python}
# list all needed imports here
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

## TODO: Predict price 
...determine the cut based on the diamond characteristics (including price). Only if the price is approximately correct and the cut can be predicted correctly, the owner considers the price of the diamond to be fair and buys it.

### DATASET DESCRIPTION

**price** price in US dollars (\$ 326 --\$18,823)

**carat** weight of the diamond (0.2--5.01)

**cut** quality of the cut (Fair, Good, Very Good, Premium, Ideal)

**color** diamond colour, from J (worst) to D (best)

**clarity** a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))

**x** length in mm (0--10.74)

**y** width in mm (0--58.9)

**z** depth in mm (0--31.8)

**depth** total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)

**table** width of top of diamond relative to widest point (43--95)

```{python}
# This is how the data needs to be inserted!

datapath = "data/diamonds_FSAI_SoSe22.csv"
raw_df = pd.read_csv(datapath, index_col = 0)
raw_df
```

```{python}
raw_df.columns
```

```{python}
raw_df.values
```

```{python}
raw_df.shape
```

```{python}
len(raw_df)
```

```{python}
# check the general information of the dataset
raw_df.info()
```

#### 'cut', 'color' and 'clarity' are categorical variables, the rest are numerical variables.

```{python}
raw_df.describe()
```

#### x, y, z has some values =  0, which means some diamonds have insufficient information and might effect to accuracy of the prediction => should be removed

```{python}
#Dropping values = 0

raw_df = raw_df.drop(raw_df[raw_df['x'] == 0].index)
raw_df = raw_df.drop(raw_df[raw_df['y'] == 0].index)
raw_df = raw_df.drop(raw_df[raw_df['z'] == 0].index)
```

```{python}
raw_df.describe()
```

```{python}
raw_df.shape
```

#### 45849 - 45832 = 17 rows with 0 values were deleted

```{python}
# plot price
raw_df.price.plot.hist()
```

```{python}
raw_df.isnull().sum()
```

#### the dataset has no null values => no need to process missing or unknown data

```{python}
raw_df['cut'].value_counts()
```

```{python}
# histplot or displot: show the numbers of each type of clarity
sns.histplot(raw_df["cut"], bins=10, alpha=0.5)
plt.show()
```

```{python}
# # plot cut in relation to price
f, axes = plt.subplots(1, 2, figsize=(10, 8)) #sharey=True

#barplots: show the mean values of each type of cut
sns.barplot(x="cut",y="price",data=raw_df, ax=axes[0], palette="magma")

#countplots: count the numbers of each type of cut
sns.countplot(x="cut",data=raw_df, ax=axes[1], palette="magma")

plt.show()
```

```{python}
raw_df['color'].value_counts()
```

```{python}
# histplot or displot: show the numbers of each type of clarity
sns.histplot(raw_df["color"], bins=10, alpha=0.5)
plt.show()
```

```{python}
# plot color in relation to price
f, axes = plt.subplots(1, 2, figsize=(10, 8)) #sharey=True

#barplots
sns.barplot(x="color",y="price",data=raw_df, ax=axes[0], palette="magma")

#countplots
sns.countplot(x="color",data=raw_df, ax=axes[1], palette="magma")

plt.show()
```

```{python}
raw_df['clarity'].value_counts()
```

```{python}
# histplot or displot: show the numbers of each type of clarity
sns.histplot(raw_df["clarity"], bins=10, alpha=0.5)
plt.show()
```

```{python}
# plot clarity in relation to price
f, axes = plt.subplots(1, 2, figsize=(10, 8)) #sharey=True

#barplots
sns.barplot(x="clarity",y="price",data=raw_df, ax=axes[0], palette="magma")

#countplots
sns.countplot(x="clarity",data=raw_df, ax=axes[1], palette="magma")

plt.show()
```

```{python}
# plot x, y, z, table, depth in relation to price to check outliers
sns.set_palette("afmhot")
cols = ['x','y', 'z', 'table', 'depth']
c = 0

fig, axs = plt.subplots(ncols = len(cols), figsize=(20,7))

for i in cols:
    sns.scatterplot(data = raw_df, x = raw_df['price'], y = raw_df[i], ax = axs[c])
    c+=1
```

#### thresholds to remove outliers  
Keep all values of x because there are no extrem outliers  
keep only y values that are < 30  
keep only z values that are < 10  
keep only table values that are < 80 and > 25  
keep only depth values that are < 75 and > 45

```{python}
# remove outliers from raw_df
raw_df = raw_df[(raw_df['y'] < 30)]
raw_df = raw_df[(raw_df['z'] < 10)]
raw_df = raw_df[(raw_df['table'] < 80) & (raw_df['table'] > 25)]
raw_df = raw_df[(raw_df['depth'] < 75) & (raw_df['depth'] > 45)]

raw_df.shape
```

```{python}
sns.set_palette("afmhot")
cols = ['x','y', 'z', 'table', 'depth']
c = 0

fig, axs = plt.subplots(ncols = len(cols), figsize=(20,7))

for i in cols:
    sns.scatterplot(data = raw_df, x = raw_df['price'], y = raw_df[i], ax = axs[c])
    c+=1
```

#### As 'cut', 'color', 'clarity' are categogy type, they are needed to transformed to numeric type. Because their values are ordinal in nature, they should be ordinally enconded.
- cut quality (Fair, Good, Very Good, Premium, Ideal)
- color from J (worst) to D (best)
- clarity (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))

```{python}
# ordinal encode 'cut', 'color', 'clarity'
raw_df_nummeric = raw_df.copy()
raw_df_nummeric['cut'] = raw_df_nummeric['cut'].map({'Fair':1,'Good':2,'Very Good':3,'Premium':4,'Ideal':5})
raw_df_nummeric['color'] = raw_df_nummeric['color'].map({'J':1,'I':2,'H':3,'G':4,'F':5,'E':6,'D':7})
raw_df_nummeric['clarity'] = raw_df_nummeric['clarity'].map({'I1':1, 'SI2':2, 'SI1':3, 'VS2':4, 'VS1':5, 'VVS2':6, 'VVS1':7, 'IF':8})
```

```{python}
raw_df_nummeric
```

```{python}
raw_df_nummeric.corr()
```

```{python}
figure, ax = plt.subplots(figsize=(12,10))

sns.heatmap(raw_df_nummeric.corr(), annot = True)
```

#### there is a high correlation between x, y, z, carat to the price


## Predicting the price (Regression task)


## Linear Regression

```{python}
df_reg = raw_df_nummeric.copy()
```

```{python}
import operator

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from sklearn.model_selection import train_test_split
```

```{python}
independent_reg = df_reg.drop("price", axis=1)
dependent_reg = df_reg['price']
```

```{python}
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(independent_reg, dependent_reg, test_size=0.2, 
                                                                    random_state=42)
```

```{python}
LR_model = LinearRegression()
LR_model.fit(X_train_reg, y_train_reg)
y_pred_reg = LR_model.predict(X_test_reg)
```

```{python}
# Error measurement
mse = mean_squared_error(y_test_reg, y_pred_reg)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test_reg, y_pred_reg)
print("MAE : "+ str(mae))
r2 = r2_score(y_test_reg, y_pred_reg)
print("R2  : "+ str(r2))
```

```{python}
plt.figure(figsize = (4,4))

plt.plot(y_test_reg, y_test_reg, color="r", label ="expected")
plt.scatter(y_pred_reg, y_test_reg, marker='x', color="C2", label ="predicted")
plt.legend()
plt.xlabel("all features")
plt.ylabel("price")
plt.title("Expected VS. Predicited")

plt.show()
```

### Predict the price based only on "carat", "x", "y", "z" (Linear Regression)

```{python}
independent_reg2 = df_reg[['carat', 'x', 'y', 'z']]
dependent_reg2 = df_reg['price']
```

```{python}
independent_reg2
```

```{python}
X_train_reg2, X_test_reg2, y_train_reg2, y_test_reg2 = train_test_split(independent_reg2, dependent_reg2, test_size=0.2, 
                                                                    random_state=42)
```

```{python}
LR_model2 = LinearRegression()
LR_model2.fit(X_train_reg2, y_train_reg2)
y_pred_reg2 = LR_model2.predict(X_test_reg2)
```

```{python}
# Error measurement
mse = mean_squared_error(y_test_reg2, y_pred_reg2)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test_reg2, y_pred_reg2)
print("MAE : "+ str(mae))
r2 = r2_score(y_test_reg2, y_pred_reg2)
print("R2  : "+ str(r2))
```

### The prediction of price based only on carat, x, y, z gives lower R2 and higher errors


## K Nearest Neighbor Regression

```{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn import metrics
```

```{python}
#Train Model and Predict
k = 6  
KNN_reg = KNeighborsRegressor(n_neighbors = k).fit(X_train_reg,y_train_reg)
y_pred_KNNR = KNN_reg.predict(X_test_reg)
```

```{python}
# Error measurement
mse = mean_squared_error(y_test_reg, y_pred_KNNR)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test_reg, y_pred_KNNR)
print("MAE : "+ str(mae))
r2 = r2_score(y_test_reg, y_pred_KNNR)
print("R2  : "+ str(r2))
```

### Testing with k = 4, 5, 6, 7 in which k = 6 gives the highest R2, less errors


## Random Forest Regressor

```{python}
import warnings
warnings.filterwarnings('ignore')

# Random Forest Regression
from sklearn.ensemble import RandomForestRegressor

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
```

```{python}
rfr = RandomForestRegressor(max_depth = 15, random_state = 42) # R2 increased when max_depth increase
rfr.fit(X_train_reg, y_train_reg)
y_pred_RFR = rfr.predict(X_test_reg)

print("MAE: ",mean_absolute_error(y_test_reg, y_pred_RFR))

print("R2: ", r2_score(y_test_reg, y_pred_RFR))

print("MSE: ", mean_squared_error(y_test_reg, y_pred_RFR))
```

```{python}
fig, ax = plt.subplots(figsize=(7, 7))
ax.plot(y_test_reg, y_test_reg, label="Expected", color = "red")
ax.scatter(y_pred_RFR, y_test_reg, label = "Predicted")

ax.set(xlabel='Predicted', ylabel='Acutal',
       title='Relation: Predicted vs. Actual Value')

ax.grid()
ax.legend()

plt.show()
```

### dataset with ordinal encoding for carat, x, y, z give better results than dataset with onehot encoding


## Neural Network Regression

```{python}
from sklearn.neural_network import MLPRegressor
```

```{python}
NN_reg = MLPRegressor(random_state=1, max_iter=300).fit(X_train_reg, y_train_reg)
y_pred_NNR = NN_reg.predict(X_test_reg)
```

```{python}
# Error measurement
mse = mean_squared_error(y_test_reg, y_pred_NNR)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test_reg, y_pred_NNR)
print("MAE : "+ str(mae))
r2_ = r2_score(y_test_reg, y_pred_NNR)
print("R2  : "+ str(r2))
```

## Finding hyperparameters using Grid Search


### As Randon Forest Regression and Neural Network Regression give highest R2 and lowest errors, we will do Grid Search the find best hyperparameters of these 2 models and compare their performance to choose the best model for our problem

```{python}
# Grid search
from sklearn.model_selection import GridSearchCV
```

### Grid search for Randon Forest Regression

```{python}
#Create the parameter grid based on the results of random search 
rfr_param_grid = {
    'bootstrap': [True, False],
    'max_depth': [5,50,100],
    'max_features': [5,10,20,40,None],
    'min_samples_leaf': [1,10,50,100],
    'min_samples_split': [5,10],
    'n_estimators': [0,50,100], 
    'random_state' : [0,20,40,60]
}
# Create a based model
rfr_GridSearch = RandomForestRegressor()

# Instantiate the grid search model
rfr_grid_search = GridSearchCV(estimator = rfr_GridSearch, param_grid = rfr_param_grid, 
                               cv = 3, n_jobs = -1, verbose = 2, scoring=['r2', 'neg_mean_absolute_error'],
                               refit='neg_mean_absolute_error')

# Test the Model on different grid parameter
rfr_grid_search.fit(X_train_reg, y_train_reg)
print(rfr_grid_search.best_params_)
```

### Predict the price with the created model

```{python}
rfr_best = rfr_grid_search.best_estimator_
y_pred_best_RFR = rfr_best.predict(X_test_reg)

print("MAE: ",mean_absolute_error(y_test_reg, y_pred_best_RFR))
print("R2: ", r2_score(y_test_reg, y_pred_best_RFR))
print("MSE: ", mean_squared_error(y_test_reg, y_pred_best_RFR))
```

```{python}
fig, ax = plt.subplots(figsize=(7, 7))
ax.plot(y_test_reg, y_test_reg, label="Expected", color = "red")
ax.scatter(y_pred_best_RFR, y_test_reg, label = "Predicted")

ax.set(xlabel='Predicted', ylabel='Acutal',
       title='Relation: Predicted vs. Actual Value')

ax.grid()
ax.legend()

plt.show()
```

### Grid search for Neural Network Regression

```{python}
#Create the parameter grid based on the results of random search 
NNR_param_grid = {
    'hidden_layer_sizes': [100, 200, 400],
    #'learning_rate': ['constant','invscaling', 'adaptive'],
    'max_iter': [100,200,400,500],
    #'early_stopping': [True, False],
    #'activation': ['identity', 'logistic','tanh', 'relu'], 
    'random_state' : [0,20,40,60]
}
# Create a based model
NNR_GridSearch = MLPRegressor()

# Instantiate the grid search model
NNR_grid_search = GridSearchCV(estimator = NNR_GridSearch, param_grid = NNR_param_grid, 
                               
                               cv = 3, n_jobs = -1, verbose = 2, scoring=['r2', 'neg_mean_absolute_error'],
                               refit='neg_mean_absolute_error')

# Test the Model on different grid parameter
NNR_grid_search.fit(X_train_reg, y_train_reg)
print(NNR_grid_search.best_params_)
```

### Predict the price with the created model

```{python}
NNR_best = NNR_grid_search.best_estimator_
y_pred_best_NNR = NNR_best.predict(X_test_reg)

print("MAE: ",mean_absolute_error(y_test_reg, y_pred_best_NNR))
print("R2: ", r2_score(y_test_reg, y_pred_best_NNR))
print("MSE: ", mean_squared_error(y_test_reg, y_pred_best_NNR))
```

# classification task: predict the cut


## K Nearest Neighbor Classifier

```{python}
df_clf = raw_df_nummeric.copy()
```

```{python}
X_clf = df_clf.drop(['cut'], axis = 1)
y_clf = df_clf['cut']

X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.20, random_state = 42)
```

```{python}
k = 7  
KNN_clf = KNeighborsClassifier(n_neighbors = k).fit(X_train_clf,y_train_clf)
y_pred_KNNC = KNN_clf.predict(X_test_clf)
```

```{python}
print("Accuracy of model at K=7 is", metrics.accuracy_score(y_test_clf, y_pred_KNNC))
print (f"Score on known data: {KNN_clf.score(X_train_clf, y_train_clf)}")
print (f"Score on unknown data: {KNN_clf.score(X_test_clf, y_test_clf)}")
```

### k =  7 gives highest accuracy for KNN classifier model

```{python}
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test_clf, y_pred_KNNC))
```

## Naive Bayes Classifier

```{python}
from sklearn.naive_bayes import GaussianNB
```

```{python}
GNB = GaussianNB()
GNB_clf = GNB.fit(X_train_clf, y_train_clf)
y_pred_GNB = GNB_clf.predict(X_test_clf)
```

```{python}
print (f"Score on known data: {GNB_clf.score(X_train_clf, y_train_clf)}")
print (f"Score on unknown data: {GNB_clf.score(X_test_clf, y_test_clf)}")
```

```{python}
print(confusion_matrix(y_test_clf, y_pred_GNB))
```

## Randon Forest Classification

```{python}
RF_clf = RandomForestClassifier(max_depth = 20, random_state = 42)
RF_clf.fit(X_train_clf, y_train_clf)
y_pred_RFC = RF_clf.predict(X_test_clf)
```

```{python}
print (f"Score on known data: {RF_clf.score(X_train_clf, y_train_clf)}")
print (f"Score on unknown data: {RF_clf.score(X_test_clf, y_test_clf)}")
```

```{python}
# Error measurement
mse = mean_squared_error(y_test_clf, y_pred_RFC)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test_clf, y_pred_RFC)
print("MAE : "+ str(mae))
r2 = r2_score(y_test_clf, y_pred_RFC)
print("R2  : "+ str(r2))
```

```{python}
print(confusion_matrix(y_test_clf, y_pred_RFC))
```

## Grid search for Randon Forest Classification

```{python}
#Create the parameter grid based on the results of random search 
RFC_param_grid = {
    'bootstrap': [True, False],
    'max_depth': [5,50,100],
    'max_features': [5,10,20,40,None],
    'min_samples_leaf': [1,10,50,100],
    'min_samples_split': [5,10],
    'n_estimators': [0,50,100], 
    'random_state' : [0,20,40,60]
}
# Create a based model
RFC_GridSearch = RandomForestClassifier()

# Instantiate the grid search model
RFC_grid_search = GridSearchCV(estimator = RFC_GridSearch, param_grid = RFC_param_grid, 
                               cv = 3, n_jobs = -1, verbose = 2, scoring=['r2', 'neg_mean_absolute_error'],
                               refit='neg_mean_absolute_error')

# Test the Model on different grid parameter
RFC_grid_search.fit(X_train_clf, y_train_clf)
print(RFC_grid_search.best_params_)
```

## Support Vector Classification

```{python}
import sklearn
from sklearn import svm # Support Vector Machine
```

```{python}
print(sklearn.__version__)
```

```{python}
svc_model_linear = svm.LinearSVC(C=1.0) #SVC = Support Vector Classifier
svc_model_linear.fit(X_train_clf, y_train_clf)
y_pred_SVC = svc_model_linear.predict(X_test_clf)
```

```{python}
print (f"Score on known data: {svc_model_linear.score(X_train_clf, y_train_clf)}")
print (f"Score on unknown data: {svc_model_linear.score(X_test_clf, y_test_clf)}")
```

```{python}
# Error measurement
mse = mean_squared_error(y_test_clf, y_pred_SVC)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test_clf, y_pred_SVC)
print("MAE : "+ str(mae))
r2 = r2_score(y_test_clf, y_pred_SVC)
print("R2  : "+ str(r2))
```

```{python}
print(confusion_matrix(y_test_clf, y_pred_SVC))
```
