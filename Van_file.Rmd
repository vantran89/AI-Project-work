---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- #region -->
# Group F

Groupmembers are:


|Gender|Firstname|Surname|Email| Study program| Semester| Student ID|
|:----:|:--------|:------|:----|:----------|:------------|:--------|
| f | Cam Van | Tran Thi | cam.tranthi@student.uni-siegen.de | HCI | 4 | 1542660 |
| f |.....|.....|.....| HCI |.....|.....|
| m |.....|.....|.....| HCI |.....|.....|

    
**Course**: First steps into Artificial Intelligence: Organization & Introduction

**Examiner**: Prof. Dr. Thomas Ludwig

**Semester**: Summersemester 2022

**Submission date**: 31. July 2022

**City**: Siegen, Germany

<!-- #endregion -->

## Imports
Following imports and addtional installtions are needed:

```{python}
# If there are additional libraries needed to run the following code install them here

```

```{python}
# list all needed imports here
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

## TODO: Predict price using other characteristics (cut, carat, color, etc)
...determine the cut based on the diamond characteristics (including price). Only if the price is approximately correct and the cut can be predicted correctly, the owner considers the price of the diamond to be fair and buys it.

### DATASET DESCRIPTION

**price** price in US dollars (\$ 326 --\$18,823)

**carat** weight of the diamond (0.2--5.01)

**cut** quality of the cut (Fair, Good, Very Good, Premium, Ideal)

**color** diamond colour, from J (worst) to D (best)

**clarity** a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))

**x** length in mm (0--10.74)

**y** width in mm (0--58.9)

**z** depth in mm (0--31.8)

**depth** total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)

**table** width of top of diamond relative to widest point (43--95)

```{python}
# This is how the data needs to be inserted!

datapath = "data/diamonds_FSAI_SoSe22.csv"
raw_df = pd.read_csv(datapath, index_col = 0)
raw_df
```

```{python}
raw_df.columns
```

```{python}
raw_df.T
```

```{python}
raw_df.values[0]
```

```{python}
raw_df.iloc[0]
```

```{python}
raw_df.loc[0]
```

```{python}
raw_df.shape
```

```{python}
len(raw_df)
```

```{python}
# check the general information of the dataset
raw_df.info()
```

#### 'cut', 'color' and 'clarity' are categorical variables, the rest are numerical variables.

```{python}
raw_df.describe()
```

#### x, y, z has some values =  0, which means some diamonds have insufficient information and might effect to accuracy of the prediction => should be removed

```{python}
#Dropping values = 0

raw_df = raw_df.drop(raw_df[raw_df['x'] == 0].index)
raw_df = raw_df.drop(raw_df[raw_df['y'] == 0].index)
raw_df = raw_df.drop(raw_df[raw_df['z'] == 0].index)
```

```{python}
raw_df.describe()
```

```{python}
raw_df.shape
```

#### 45849 - 45832 = 17 rows with 0 values were deleted

```{python}
raw_df.values
```

```{python}
raw_df['price'].describe()
```

```{python}
# plot price
raw_df.price.plot.hist()
```

```{python}
raw_df.notnull().sum()
```

```{python}
raw_df.isnull().sum()
```

#### the dataset has no null values => no need to process missing or unknown data

```{python}
len(raw_df)-len(raw_df.dropna())
```

```{python}
raw_df['cut'].value_counts()
```

```{python}
# histplot or displot: show the numbers of each type of clarity
sns.histplot(raw_df["cut"], bins=10, alpha=0.5)
plt.show()
```

```{python}
# # plot cut in relation to price
f, axes = plt.subplots(1, 2, figsize=(10, 8)) #sharey=True

#barplots: show the mean values of each type of cut
sns.barplot(x="cut",y="price",data=raw_df, ax=axes[0], palette="magma")

#countplots: count the numbers of each type of cut
sns.countplot(x="cut",data=raw_df, ax=axes[1], palette="magma")

plt.show()
```

```{python}
raw_df['color'].value_counts()
```

```{python}
# histplot or displot: show the numbers of each type of clarity
sns.histplot(raw_df["color"], bins=10, alpha=0.5)
plt.show()
```

```{python}
# plot color in relation to price
f, axes = plt.subplots(1, 2, figsize=(10, 8)) #sharey=True

#barplots
sns.barplot(x="color",y="price",data=raw_df, ax=axes[0], palette="magma")

#countplots
sns.countplot(x="color",data=raw_df, ax=axes[1], palette="magma")

plt.show()
```

```{python}
raw_df['clarity'].value_counts()
```

```{python}
# histplot or displot: show the numbers of each type of clarity
sns.histplot(raw_df["clarity"], bins=10, alpha=0.5)
plt.show()
```

```{python}
# plot clarity in relation to price
f, axes = plt.subplots(1, 2, figsize=(10, 8)) #sharey=True

#barplots
sns.barplot(x="clarity",y="price",data=raw_df, ax=axes[0], palette="magma")

#countplots
sns.countplot(x="clarity",data=raw_df, ax=axes[1], palette="magma")

plt.show()
```

```{python}
cut_group = raw_df.groupby('cut').count()
cut_group
```

```{python}
cut_group = raw_df.groupby('cut').mean()
cut_group
```

```{python}
type(cut_group)
```

```{python}
color_group = raw_df.groupby('color').count()
color_group
```

```{python}
# check for outliers of the cut
sns.pairplot(raw_df, hue = 'cut')
```

```{python}
# plot x, y, z, table, depth in relation to price to check outliers
sns.set_palette("afmhot")
cols = ['x','y', 'z', 'table', 'depth']
c = 0

fig, axs = plt.subplots(ncols = len(cols), figsize=(20,7))

for i in cols:
    sns.scatterplot(data = raw_df, x = raw_df['price'], y = raw_df[i], ax = axs[c])
    c+=1
```

#### thresholds to remove outliers  
Keep all values of x because there are no extrem outliers  
keep only y values that are < 30  
keep only z values that are < 10  
keep only table values that are < 80 and > 25  
keep only depth values that are < 75 and > 45

```{python}
# remove outliers from raw_df
raw_df = raw_df[(raw_df['y'] < 30)]
raw_df = raw_df[(raw_df['z'] < 10)]
raw_df = raw_df[(raw_df['table'] < 80) & (raw_df['table'] > 25)]
raw_df = raw_df[(raw_df['depth'] < 75) & (raw_df['depth'] > 45)]

raw_df.shape
```

```{python}
sns.set_palette("afmhot")
cols = ['x','y', 'z', 'table', 'depth']
c = 0

fig, axs = plt.subplots(ncols = len(cols), figsize=(20,7))

for i in cols:
    sns.scatterplot(data = raw_df, x = raw_df['price'], y = raw_df[i], ax = axs[c])
    c+=1
```

#### As 'cut', 'color', 'clarity' are categogy type, they are needed to transformed to numeric type. Because their values are ordinal in nature, they should be ordinally enconded.
- cut quality (Fair, Good, Very Good, Premium, Ideal)
- color from J (worst) to D (best)
- clarity (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))

```{python}
# ordinal encode 'cut', 'color', 'clarity'
raw_df_nummeric = raw_df.copy()
raw_df_nummeric['cut'] = raw_df_nummeric['cut'].map({'Fair':0,'Good':1,'Very Good':2,'Premium':3,'Ideal':4})
raw_df_nummeric['color'] = raw_df_nummeric['color'].map({'J':0,'I':1,'H':2,'G':3,'F':4,'E':5,'D':6})
raw_df_nummeric['clarity'] = raw_df_nummeric['clarity'].map({'I1':0, 'SI2':1, 'SI1':2, 'VS2':3, 'VS1':4, 'VVS2':5, 'VVS1':6, 'IF':7})
```

```{python}
raw_df_nummeric
```

```{python}
raw_df_nummeric.corr()
```

```{python}
figure, ax = plt.subplots(figsize=(12,10))

sns.heatmap(raw_df_nummeric.corr(), annot = True)
```

#### there is a high correlation between x, y, z, carat to the price

```{python}
import operator

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

from sklearn.model_selection import train_test_split
```

```{python}
# X = raw_df_nummeric.drop('price', axis = 1)
# y = raw_df_nummeric['price']

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
```

```{python}
independent = raw_df_nummeric["carat"]
dependent = raw_df_nummeric["price"]
```

```{python}
model = LinearRegression()

# model.fit (<Training data>, <Target data>)
model.fit(independent.values.reshape(-1,1), dependent)

# predicts the wanted value (dependent) based on the fitted model and the available data (independent)
model_pred = model.predict(independent.values.reshape(-1,1))
```

```{python}
plt.figure(figsize = (8,8))

plt.scatter(independent, dependent)

# draw the regression line
Z = np.linspace(0,100, 100)
T = model.intercept_ + model.coef_*Z
plt.plot(Z,T, color = "red")


plt.xlabel("carat")
plt.ylabel("price")
plt.title("carat and price")

plt.ylim(0, max(dependent)+1)
plt.xlim(0, max(independent)+1)

#plt.savefig("price-VS-carat.png")
plt.show()

print ('The R^2 of linear regression is: ',r2_score(dependent.values, model_pred))
```

```{python}
# predict price using independent x
independent_x = raw_df_nummeric["x"]
# model.fit (<Training data>, <Target data>)
model.fit(independent_x.values.reshape(-1,1), dependent)

# predicts the wanted value (dependent) based on the fitted model and the available data (independent)
model_pred_x = model.predict(independent_x.values.reshape(-1,1))
```

```{python}
plt.figure(figsize = (8,8))

plt.scatter(independent_x, dependent)

# draw the regression line
Z = np.linspace(0,100, 100)
T = model.intercept_ + model.coef_*Z
plt.plot(Z,T, color = "red")


plt.xlabel("x")
plt.ylabel("price")
plt.title("x and price")

plt.ylim(0, max(dependent)+1)
plt.xlim(0, max(independent_x)+1)

#plt.savefig("price-VS-x.png")
plt.show()

print ('The R^2 of linear regression is: ',r2_score(dependent.values, model_pred_x))
```

```{python}
# predict price using independent y
independent_y = raw_df_nummeric["y"]
# model.fit (<Training data>, <Target data>)
model.fit(independent_y.values.reshape(-1,1), dependent)

# predicts the wanted value (dependent) based on the fitted model and the available data (independent)
model_pred_y = model.predict(independent_y.values.reshape(-1,1))
```

```{python}
plt.figure(figsize = (8,8))

plt.scatter(independent_y, dependent)

# draw the regression line
Z = np.linspace(0,100, 100)
T = model.intercept_ + model.coef_*Z
plt.plot(Z,T, color = "red")


plt.xlabel("y")
plt.ylabel("price")
plt.title("y and price")

plt.ylim(0, max(dependent)+1)
plt.xlim(0, max(independent_y)+1)

#plt.savefig("y-VS-carat.png")
plt.show()

print ('The R^2 of linear regression is: ',r2_score(dependent.values, model_pred_y))
```

```{python}
# predict price using independent z
independent_z = raw_df_nummeric["z"]
# model.fit (<Training data>, <Target data>)
model.fit(independent_z.values.reshape(-1,1), dependent)

# predicts the wanted value (dependent) based on the fitted model and the available data (independent)
model_pred_z = model.predict(independent_z.values.reshape(-1,1))
```

```{python}
plt.figure(figsize = (8,8))

plt.scatter(independent_z, dependent)

# draw the regression line
Z = np.linspace(0,100, 100)
T = model.intercept_ + model.coef_*Z
plt.plot(Z,T, color = "red")


plt.xlabel("z")
plt.ylabel("price")
plt.title("z and price")

plt.ylim(0, max(dependent)+1)
plt.xlim(0, max(independent_z)+1)

#plt.savefig("z-VS-carat.png")
plt.show()

print ('The R^2 of linear regression is: ',r2_score(dependent.values, model_pred_z))
```

#### variable carat give the highest accuracy in predicting the price compared to x, y, z

```{python}
####### create Polynomial Regression model

degree = 4

poly_reg = PolynomialFeatures(degree=degree)

independent_poly = poly_reg.fit_transform(independent.values.reshape(-1,1))

lin_reg_Poly = LinearRegression()
lin_reg_Poly.fit(independent_poly, dependent)
model_poly_pred = lin_reg_Poly.predict(independent_poly)

####### plot the model

plt.figure(figsize = (8,8))
plt.scatter(independent, dependent)

# sort the independent values before line plot (but use the unsorted values for calculating )
sort_axis = operator.itemgetter(0)
# The zip() function takes iterables (can be zero or more), aggregates them in a tuple, and returns it.
sorted_zip = sorted(zip(independent,
                        model_poly_pred), 
                    key=sort_axis)
independent_sorted, model_poly_pred_sorted = zip(*sorted_zip)

plt.plot(independent_sorted, model_poly_pred_sorted, color='red')

plt.xlabel("carat")
plt.ylabel("price")
plt.title("carat and price")

plt.ylim(0, max(dependent)+1)
plt.xlim(0, max(independent)+1)
#plt.savefig("carat-VS-price.png")
plt.show()

print (f'The R^2 of polynomial regression (degree = {degree}) is: ',r2_score(dependent.values, model_poly_pred))
```

## Linear Regression

```{python}
X_train, X_test, y_train, y_test = train_test_split(independent, dependent, test_size=0.3, random_state = 42) # random_state = 
```

```{python}
LR_model = LinearRegression()
LR_model.fit(X_train.values.reshape(-1,1), y_train)
print("model is trained")
```

```{python}
y_pred = LR_model.predict(X_test.values.reshape(-1,1))
```

```{python}
plt.figure(figsize = (12,8))

plt.subplot(1,2,1)
plt.xlabel("carat")
plt.ylabel("price")
plt.title('Actual distribution')
plt.scatter(X_train, y_train, marker='x', color="C0", label="trainset")
plt.scatter(X_test, y_test, marker='x', color="C1", label="testset")
plt.legend()

# subplot no. 2
plt.subplot(1,2,2)
plt.xlabel("carat")
plt.ylabel("price")
plt.title("Predicted Linear Regression")
plt.scatter(X_train, y_train, marker='x', color="C0", label="trainset")
plt.scatter(X_test, y_pred, marker='x', color="C1", label="predicted")
plt.legend()

plt.show()
```

```{python}
plt.figure(figsize = (4,4))

plt.plot(y_test, y_test, color="r", label ="expected")
plt.scatter(y_pred, y_test, marker='x', color="C2", label ="predicted")
plt.legend()
plt.xlabel("carat")
plt.ylabel("price")
plt.title("Expected VS. Predicited")

plt.show()
```

```{python}
# Error measurement
mse = mean_squared_error(y_test, y_pred)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test, y_pred)
print("MAE : "+ str(mae))
r2 = r2_score(y_test, y_pred)
print("R2  : "+ str(r2))
```

## Polynomial Regression

```{python}
degree = 5

poly_reg = PolynomialFeatures(degree=degree)

x_train_poly = poly_reg.fit_transform(X_train.values.reshape(-1,1))

model_poly = LinearRegression()
model_poly.fit(x_train_poly, y_train)

print("Model trained")

y_pred_poly = model_poly.predict(poly_reg.fit_transform(X_test.values.reshape(-1,1)))

print("Predicitons done")
print ('The R^2 for quadratic curve is: ',r2_score(y_test, y_pred_poly))
```

```{python}
plt.figure(figsize = (12,8))

plt.subplot(1,2,1)
plt.xlabel("carat")
plt.ylabel("price")
plt.title('Actual distribution')
plt.scatter(X_train, y_train, marker='x', color="C0", label="trainset")
plt.scatter(X_test, y_test, marker='x', color="C1", label="testset")
plt.legend()

# subplot no. 2
plt.subplot(1,2,2)
plt.xlabel("carat")
plt.ylabel("price")
plt.title("Predicted Polynomial Regression")
plt.scatter(X_train, y_train, marker='x', color="C0", label = "Trainset")
plt.scatter(X_test, y_pred_poly, marker='x', color="C1", label = "Predicted")
plt.legend()

plt.show()
```

```{python}
plt.figure(figsize = (4,4))

plt.plot(y_test, y_test, color="r", label ="expected")
plt.scatter(y_pred_poly, y_test, marker='x', color="C2", label ="predicted")
plt.legend()
plt.xlabel("carat")
plt.ylabel("price")
plt.title("Expected VS. Predicited")

plt.show()
```

```{python}
# Error measurement
mse = mean_squared_error(y_test, y_pred_poly)
print("MSE : "+ str(mse))
rmse= np.sqrt(mse)
print("RMSE: "+ str(rmse))
mae = mean_absolute_error(y_test, y_pred_poly)
print("MAE : "+ str(mae))
r2 = r2_score(y_test, y_pred_poly)
print("R2  : "+ str(r2))
```

#### Polynomial Regression has lower errors and higher R2 then Linear Regression


## Random Forest Regressor

```{python}
import warnings
warnings.filterwarnings('ignore')

# Random Forest Regression
from sklearn.ensemble import RandomForestRegressor

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
```

```{python}
target = "price"

X_regression = raw_df_nummeric.drop(axis = 1, columns= [target])
y_regression = raw_df_nummeric[target]
```

```{python}
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_regression, y_regression, test_size=0.30, random_state=42)
```

```{python}
rfr = RandomForestRegressor(max_depth = 5, random_state = 42)
rfr.fit(X_train_reg, y_train_reg)
y_pred_reg = rfr.predict(X_test_reg)

print("MAE: ",mean_absolute_error(y_test_reg, y_pred_reg))

print("R2: ", r2_score(y_test_reg, y_pred_reg))

print("MSE: ", mean_squared_error(y_test_reg, y_pred_reg))
```

```{python}
fig, ax = plt.subplots(figsize=(7, 7))
ax.plot(y_test_reg, y_test_reg, label="Expected", color = "red")
ax.scatter(y_pred_reg, y_test_reg, label = "Predicted")

ax.set(xlabel='Predicted', ylabel='Acutal',
       title='Relation: Predicted vs. Actual Value')

ax.grid()
ax.legend()

plt.show()
```

#### Randon Forest give higher R2 and less errors than Polynomial Regression and Linear Regression

```{python}
raw_df_dummies = pd.get_dummies(raw_df)
```

```{python}
raw_df_dummies
```

```{python}
target = "price"

X_regression_dummies = raw_df_dummies.drop(axis = 1, columns= [target])
y_regression_dummies = raw_df_dummies[target]

X_train_reg_dummies, X_test_reg_dummies, y_train_reg_dummies, y_test_reg_dummies = train_test_split(
    X_regression_dummies, y_regression_dummies, test_size=0.30, random_state=42)

rfr2 = RandomForestRegressor(max_depth = 5, random_state = 42)
rfr2.fit(X_train_reg_dummies, y_train_reg_dummies)
y_pred_reg_dummies = rfr2.predict(X_test_reg_dummies)

print("MAE: ",mean_absolute_error(y_test_reg_dummies, y_pred_reg_dummies))

print("R2: ", r2_score(y_test_reg_dummies, y_pred_reg_dummies))

print("MSE: ", mean_squared_error(y_test_reg_dummies, y_pred_reg_dummies))
```

#### dataset with ordinal encoding for carat, x, y, z give better results than dataset with onehot encoding


# classification task: predict the cut


### Classification with the Support Vector Machine

```{python}
import sklearn
from sklearn import svm, datasets # Support Vector Machine
```

```{python}
print(sklearn.__version__)
```

```{python}
raw_df.cut.unique()
```

```{python}
sns.lmplot(data=raw_df, x="carat", y="price", hue="cut",fit_reg=False)
```

```{python}
X = raw_df[["carat", "price"]]
y = raw_df['cut']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 42)
```

```{python}
svc_model_linear = svm.LinearSVC(C=1.0) #SVC = Support Vector Classifier
svc_model_linear.fit(X_train,y_train)
```

```{python}
y_pred = svc_model_linear.predict(X_test)
y_pred
```

```{python}
y_pred_df = pd.Series(y_pred)
y_pred_df
```

```{python}
y_test
```

```{python}
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred))
```

```{python}
cur_test = X_test.copy()
cur_test["cut"] = y_test
cur_test

cur_pred = X_test.copy()
cur_pred["cut"] = y_pred
cur_pred

print("Predicted:")
sns.lmplot(data=cur_pred, y="price", x="carat", hue="cut",fit_reg=False)
plt.show()
print("Acutal:")
sns.lmplot(data=cur_test, y="price", x="carat", hue="cut",fit_reg=False)
plt.show()
```

### classification task with ordinal encoding of the cut

```{python}
raw_df_nummeric.cut.unique()
```

```{python}
X2 = raw_df_nummeric[["carat", "price"]]
y2 = raw_df_nummeric['cut']

X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.30, random_state = 42)
```

```{python}
svc_model_linear2 = svm.LinearSVC(C=1.0) #SVC = Support Vector Classifier
svc_model_linear2.fit(X_train2,y_train2)
```

```{python}
y_pred2 = svc_model_linear2.predict(X_test2)
y_pred2
```

```{python}
print(confusion_matrix(y_test2, y_pred2))
```

```{python}
print("MAE: ",mean_absolute_error(y_test2, y_pred2))

print("R2: ", r2_score(y_test2, y_pred2))

print("MSE: ", mean_squared_error(y_test2, y_pred2))
```

### Randon Forest Classification

```{python}
target= "cut"

X_classification = raw_df_nummeric.drop(axis = 1, columns= [target])
y_classification = raw_df_nummeric[target]
```

```{python}
X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(
    X_classification, y_classification, test_size=0.30, random_state=42)
```

```{python}
rfc = RandomForestClassifier(max_depth = 5, random_state = 42)
rfc.fit(X_train_cl, y_train_cl)
y_pred_cl = rfc.predict(X_test_cl)

print (f"Score on known data: {rfc.score(X_train_cl, y_train_cl)}")
print (f"Score on unknown data: {rfc.score(X_test_cl, y_test_cl)}")
```

```{python}
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC

SVC(random_state=0) 
ConfusionMatrixDisplay.from_estimator(rfc, X_test_cl, y_test_cl)

# plt.grid(visible=None)
plt.show()
```
